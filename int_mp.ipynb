{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "int_mp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMjLSVHoAdwHS8vdttrPtxm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RazerRaymond/CrunchBase_Predictor/blob/main/int_mp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPdBNkB4wQg"
      },
      "source": [
        "# **Early stage startup model**\n",
        "*This is an model for early start-up evaluation for interview purposes at Miracle Plus China*\n",
        "\n",
        "Data used in model grabbed from GitHub user @notpeter from [Link](https://github.com/notpeter/crunchbase-data).\n",
        "\n",
        "All relavent data are copyright reserved for Crunchbase. Do not redistribute or use for any business uses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dk8P-OVKKPMw"
      },
      "source": [
        "## Rationales\n",
        "Because It's hard to compute ROI given that dataset has no IPO data (Crunchbase actually has that dataset, but it would require enterprise access), I decice to work with acquired firms only. Not only this gives a more accruate estimates, but also reduce the time to train the model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyhRxKGgKciH"
      },
      "source": [
        "### Possible Flaws\n",
        "首先sample是有些biased，因为我data cleaning用到的一些techique是drop null values， 大公司或者更好的startups更应该有完整的profile所以选到的有bias\n",
        "其次companies的categories我只选了第一个，实际上可以用list 列出所有的再one hot encoding但是这样会比较麻烦，sample数也不太够\n",
        "最后的regression结果可以根据公司需求的roi来选择是否投资，我觉得3-5的cutoff都很不错"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUHYGV1REyQ6"
      },
      "source": [
        "### Data Cleaning\n",
        "In order to run and fit the data, we have to make sure our data is numerical.\n",
        "Csv doesn't automatically do that, and we also need to clean all the Null values of our interest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnPWHkyJ6RTA"
      },
      "source": [
        "# libraries needed\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime\n",
        "import csv\n",
        "from scipy.stats import skewnorm "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc8aGObw-w8S"
      },
      "source": [
        "# import datas\n",
        "#inv_df = pd.read_csv('https://raw.githubusercontent.com/notpeter/crunchbase-data/master/investments.csv')\n",
        "\n",
        "# Data Cleaning - parse all time object to datetime type\n",
        "date_parser = lambda c: pd.to_datetime(c, format='%Y-%m-%d', errors='coerce')\n",
        "acq_df = pd.read_csv('https://raw.githubusercontent.com/notpeter/crunchbase-data/master/acquisitions.csv', parse_dates=['acquired_at'], date_parser=date_parser)\n",
        "comp_df = pd.read_csv('https://raw.githubusercontent.com/notpeter/crunchbase-data/master/companies.csv', parse_dates=['first_funding_at', 'last_funding_at'], date_parser=date_parser)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H90UJe0dBrdC"
      },
      "source": [
        "# Data Cleaning\n",
        "# To clean the data, we manually convert all time columns of interest that is null\n",
        "acq_l = ['price_amount','acquired_at']\n",
        "comp_l = ['category_list', 'status', 'country_code', 'funding_rounds', 'first_funding_at', 'last_funding_at']\n",
        "acq_df = acq_df.dropna(subset=acq_l)\n",
        "comp_df = comp_df.dropna(subset=comp_l)\n",
        "\n",
        "comp_df = comp_df[comp_df.funding_total_usd.apply(lambda x: x.isnumeric())]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAd0nl-lI8nI"
      },
      "source": [
        "Merging two dataframes to get one that we really want and train later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWt3sPP4Flc5",
        "outputId": "b8e1954f-9bd1-4b9a-f9d2-a26ef022ca55"
      },
      "source": [
        "# Merge two dataframe to get the results that we want on acquired firms\n",
        "df = pd.merge(acq_df, comp_df, left_on='company_permalink', right_on='permalink')\n",
        "inted_list = ['funding_rounds', 'funding_total_usd','category_list', 'country_code', 'status', 'first_funding_at', 'last_funding_at', 'acquired_at', 'price_amount']\n",
        "df = df[inted_list]\n",
        "df['funding_total_usd'] = df['funding_total_usd'] .astype(np.float64)\n",
        "df.info()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 1491 entries, 0 to 1490\n",
            "Data columns (total 9 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   funding_rounds     1491 non-null   int64         \n",
            " 1   funding_total_usd  1491 non-null   float64       \n",
            " 2   category_list      1491 non-null   object        \n",
            " 3   country_code       1491 non-null   object        \n",
            " 4   status             1491 non-null   object        \n",
            " 5   first_funding_at   1491 non-null   datetime64[ns]\n",
            " 6   last_funding_at    1491 non-null   datetime64[ns]\n",
            " 7   acquired_at        1491 non-null   datetime64[ns]\n",
            " 8   price_amount       1491 non-null   float64       \n",
            "dtypes: datetime64[ns](3), float64(2), int64(1), object(3)\n",
            "memory usage: 116.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poSRnjsAEmBz"
      },
      "source": [
        "### Data Enrichment\n",
        "It's biased to run simply on acquired firms, since not all invested firms (I guess only very small percent) are acquired. I decided to enrich the data set with 10000 entries, with their value now being transformed by an left-skewed normal distribution from their total investment to reflect their values for now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RY1zQY6JJ7F7"
      },
      "source": [
        "*This part has certain flaw since ROI are surely not independent of investment amount, and will draw more problems when making investment decisions. I just assumed ROI as a single consideration for the sake of simplicity.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eIM8Sz9FNWw"
      },
      "source": [
        "# select first 10000 rows and select interest value\n",
        "added = comp_df.head(10000)\n",
        "added = added[['funding_rounds', 'funding_total_usd','category_list', 'country_code', 'status', 'first_funding_at', 'last_funding_at']]\n",
        "\n",
        "#manually set date to 2016-01-01 and do cleaning\n",
        "added['acquired_at'] = datetime.datetime(2016, 1, 1, 0, 0)\n",
        "added['funding_total_usd'] = added['funding_total_usd'] .astype(np.float64)\n",
        "\n",
        "# transform using skewed normal dist\n",
        "added['price_amount'] = added.apply(lambda x : skewnorm.rvs(a=5, loc = 0.1, scale =0.25)*x.funding_total_usd, axis = 1)\n",
        "\n",
        "#update our data frame\n",
        "df = pd.concat([df,added]).drop_duplicates().reset_index(drop=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOAhTNh3rDrr"
      },
      "source": [
        "# kept the first category(flawed)\n",
        "# convert some of the key info into form of data\n",
        "# convert to time dif instead of time\n",
        "df['category'] = df['category_list'].str.split('|').str[0]\n",
        "df['invest_period'] = df['last_funding_at'] - df['first_funding_at'] \n",
        "df['invest_period'] = df['invest_period']/np.timedelta64(1,'M')\n",
        "df['payout period'] = df['acquired_at'] - df['last_funding_at'] \n",
        "df['payout period'] = df['payout period']/np.timedelta64(1,'M')\n",
        "# ROI instead of amount \n",
        "df['ROI'] = ((df['price_amount'] - df['funding_total_usd']) / df['funding_total_usd'])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KL3PJjBNmF4P"
      },
      "source": [
        "# Clean the final data set\n",
        "# hot_cc = pd.get_dummies(df['country_code'])\n",
        "# hot_st = pd.get_dummies(df['status'])\n",
        "df = df.drop(columns = ['funding_total_usd', 'acquired_at', 'first_funding_at', 'last_funding_at', 'category_list'])\n",
        "# df = df.join(hot_cc)\n",
        "# df = df.join(hot_st)\n",
        "#df = df['funding_rounds','funding_total_usd','category_list','country_code','status', 'invest_period']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCutUrGptn7I",
        "outputId": "0dbc1a22-2622-4a55-dfbf-1ea01cdc1686"
      },
      "source": [
        "# Rearrange final df's columns\n",
        "\n",
        "#list(df.columns.values)\n",
        "rear = ['category',\n",
        " 'country_code',\n",
        " 'status',\n",
        " 'funding_rounds',\n",
        " 'price_amount',\n",
        " 'invest_period',\n",
        " 'payout period',\n",
        " 'ROI']\n",
        "df = df[rear]\n",
        "df.info()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 11489 entries, 0 to 11488\n",
            "Data columns (total 8 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   category        11489 non-null  object \n",
            " 1   country_code    11489 non-null  object \n",
            " 2   status          11489 non-null  object \n",
            " 3   funding_rounds  11489 non-null  int64  \n",
            " 4   price_amount    11489 non-null  float64\n",
            " 5   invest_period   11489 non-null  float64\n",
            " 6   payout period   11489 non-null  float64\n",
            " 7   ROI             11489 non-null  float64\n",
            "dtypes: float64(4), int64(1), object(3)\n",
            "memory usage: 718.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlvXzceovX9q"
      },
      "source": [
        "### Training\n",
        "Now we try to fit a neural network to predict ROI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHTAXd49vWiy",
        "outputId": "b33c8c36-c0a3-4692-f884-8fc41d57b0e2"
      },
      "source": [
        "#import libs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Encoding the data\n",
        "df = pd.get_dummies(df)\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.1)\n",
        "\n",
        "\n",
        "y_train = train['ROI'].values\n",
        "y_test = test['ROI'].values\n",
        "train.drop('ROI', axis=1)\n",
        "test.drop('ROI', axis=1)\n",
        "x_ta = train.values\n",
        "x_te = test.values\n",
        "\n",
        "# normalize data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(x_ta)\n",
        "X_test = scaler.fit_transform(x_te)\n",
        "\n",
        "# testing alpha alpha = 0.0003\n",
        "# regr_cv = RidgeCV(alphas=[0.0003, 0.0001, 0.0005])\n",
        "# model_cv = regr_cv.fit(X_train, y_train)\n",
        "# model_cv.alpha_\n",
        "ridge = linear_model.Ridge(alpha=0.0003, normalize=False)\n",
        "ridge.fit(X_train, y_train)\n",
        "pred_train_rr= ridge.predict(X_test)\n",
        "print(np.sqrt(mean_squared_error(y_test,pred_train_rr)))\n",
        "print(r2_score(y_test, pred_train_rr))\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56.055178726179086\n",
            "0.9401277893629795\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBN8WPl-yNFU"
      },
      "source": [
        "*The interesting train test split is inspired by [here](https://stackoverflow.com/questions/24147278/how-do-i-create-test-and-train-samples-from-one-dataframe-with-pandas), as well as one hot encoding [here](https://stackoverflow.com/questions/34007308/linear-regression-analysis-with-string-categorical-features-variables)\n",
        "and standardization CV picking from [here](https://chrisalbon.com/machine_learning/linear_regression/selecting_best_alpha_value_in_ridge_regression/)*\n"
      ]
    }
  ]
}